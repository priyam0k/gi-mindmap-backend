As a Specialist Actuarial Note Builder and Exam Coach for SP8: General Insurance Pricing Specialist Principles, let's delve into the practical considerations surrounding the Burning Cost Approach. Understanding these nuances is crucial for your application skills in the exam, as you'll often encounter scenarios where data limitations or specific business contexts necessitate careful judgement.

### **The Burning Cost Approach: Practical Considerations in General Insurance Pricing**

The Burning Cost Approach is a fundamental method in general insurance pricing, particularly useful in situations where detailed data for granular analysis might be scarce or unreliable. It's a core pricing methodology you must master for SP8.

#### **1\. Definition and Core Idea**

At its heart, the Burning Cost Approach calculates the risk premium as the actual cost of claims incurred during a past period, expressed as an annual rate per unit of exposure. Conceptually, it focuses on the total claims rather than explicitly separating frequency and severity. The fundamental formula for the burning cost premium is `BCP = Sum(Claims) / Total Exposed to Risk`.

However, this simplicity comes with a need for careful practical application, as the raw burning cost often needs significant adjustments to be a reliable indicator for future rates.

#### **2\. Data Requirements and Quality**

The quality and quantity of data are paramount in any ratemaking process, including the burning cost approach. While the burning cost approach is known for needing "relatively little data" compared to more sophisticated methods, this does not negate the importance of what data *is* available.

* **Type of Data**: The method primarily relies on aggregate claims data. Unlike the frequency/severity approach, it doesn't necessitate detailed individual claim information. However, policy data is still needed to calculate exposure and identify characteristics of risk groups, and claims data is essential for estimating ultimate costs.  
* **Source of Data**: Internal historical data is generally preferred due to its relevance. However, for new companies or lines of business with insufficient internal data, external sources such as market claims data, publicly available information (e.g., flood-prone areas), or competitors' rates become invaluable. Reinsurers can also be a source of data.  
* **Data Reliability and Relevance**: For the burning cost to be meaningful, the claims data should be recent, include up-to-date case estimates or other accurate information, and be based on consistent approaches to claims recording, payment, and settlement. If data storage protocols have changed historically, adjustments may be necessary to avoid distortions in the claims development pattern.  
* **Inadequate Data**: When data is incomplete, inaccurate, or sparse, compromises may be necessary, and the calculated rates may be unprofitable, uncompetitive, or lead to anti-selection. In such cases, non-statistical methods like exposure-based or simple ratio methods may be considered.

#### **3\. Adjustments and Trending**

A key practical consideration is that a crude burning cost approach, using unadjusted data, "will often end up with loss ratios higher than planned" because it "will understate the ultimate position" by ignoring various factors. Therefore, several adjustments are typically necessary to make the historical data relevant for the future policy period.

* **Inflation**: Claims data should be adjusted for past inflation to bring it to a consistent cost level and projected for future inflation. For property claims, this involves price and building cost inflation; for liability, earnings inflation and trends in court awards. Similarly, expense inflation needs to be considered as insurance is labor-intensive.  
* **Other Trends**: Beyond inflation, other trends affecting frequency and severity (e.g., changes in accident frequency, legislation, social/economic environment, structure of risk, technical changes, policy conditions, business volumes, etc.) must be accounted for. If frequency and severity cannot be analyzed separately, general trending approaches must be applied.  
* **"On-Level" Data**: Data is often put "on-level" or "as-if" (as if claims were occurring today) to ensure comparability.  
* **Consistency**: All components of the ratemaking formula (e.g., exposure, premium, loss) must be trended consistently.

#### **4\. Treatment of Large and Catastrophe Losses**

Large or unusual claims can significantly distort historical data and invalidate underlying assumptions.

* **Catastrophe Losses**: Catastrophe claims are typically omitted from the burning cost analysis because they are infrequent but high-severity events whose observed past losses may not reflect the true underlying risk. They are usually accounted for separately, often using outputs from catastrophe modelling systems.  
* **Large (Non-Catastrophe) Losses**: For large non-catastrophe losses (common in liability classes), actuaries have several approaches:  
  * Omit them from the analysis and allow for them separately in the risk premium.  
  * Truncate them at a set point and spread the cost above this level across the larger portfolio.  
  * Remove them altogether.  
  * Different trends and projection methods may apply to smaller (attritional) versus larger claims.

#### **5\. Developing Losses to Ultimate**

Unless a very crude approach is taken, historical claims information must be developed to their ultimate values. This means accounting for claims that have been reported but not fully settled (outstanding reported claims) and claims that have occurred but not yet been reported (IBNR).

* **Reserving Techniques**: Standard reserving techniques like the chain ladder method or Bornhuetter-Ferguson method can be used to calculate ultimate overall claim amounts for each development year. These methods are crucial for projecting IBNR and IBNER (incurred but not enough reported) components.  
* **Practicalities**: In practice, developing individual losses to ultimate is important for methods like frequency-severity, but for burning cost, the aggregate loss development is often the focus. When data is sparse, benchmarks or industry standards might be used for development patterns.

#### **6\. Expenses and Profit Loadings**

The burning cost approach typically derives a "risk premium," which covers only the expected cost of claims. To arrive at the final "office premium," additional loadings for expenses, profit, investment income, and reinsurance are applied.

* **Expense Analysis**: Expenses are a significant element of an insurer's outgo. They need to be analyzed by source and type to ensure suitable allowances are made. Expenses can be categorized as incurred at policy inception (e.g., commissions) or throughout the policy year (e.g., general administration). Direct expenses are attributable to a specific class, while indirect expenses (overheads) need to be apportioned using sensible methods like proportion of policies in force, premiums, or funds under management.  
* **Profit Loading**: A profit loading is built into the premium, often influenced by the insurance cycle. This can be derived from a profit target, return on capital target, or target loss/combined ratio. Actuaries may also consider more advanced approaches for setting profit loadings based on risk metrics, such as Wang's Proportional Hazards approach, which loads higher for policies with heavier tail probabilities.  
* **Reinsurance Costs**: Reinsurance costs should be incorporated, either by reducing the expected loss or by including them as an expense loading. For proportional reinsurance, explicit consideration might not be needed as premium and losses are ceded proportionally. For non-proportional, it becomes more crucial.  
* **Investment Income**: Investment income is typically treated as a deduction when determining the final premium.

#### **7\. Trade-offs: Simplicity vs. Accuracy**

The burning cost approach is valued for its **simplicity**, needing **relatively little data**, and being **quicker to perform** than other methods. This makes it practical for individual risk rating or for portfolios where data is limited.

However, a major practical disadvantage is that it can **ignore critical trends** like claims inflation and, if not properly adjusted, will **understate the ultimate position**, potentially leading to higher-than-planned loss ratios. The frequency/severity approach, by contrast, provides a better understanding of underlying data and trends, leading to more accurate results, but at the cost of greater data requirements and complexity. The choice often depends on the available data, time/resource constraints, and the specific needs of the analysis.

#### **8\. Contextual Application**

* **New Lines of Business**: The pure premium method (closely related to burning cost) is particularly appropriate for pricing new lines of business or situations where calculating premium at current rate level is difficult.  
* **Reinsurance Pricing**: The burning cost approach is often used for pricing excess of loss reinsurance contracts, especially when data is sparse, which might prohibit a frequency/severity approach. When used for reinsurance, the reinsurer applies the reinsurance terms to the historical losses after adjusting for trends and future development.  
* **Commercial Lines**: While commercial lines ratemaking often employs special techniques due to heterogeneity and credibility, the burning cost approach can be used for individual commercial risks or portfolios.

#### **9\. Market and Regulatory Influences**

* **Regulatory Scrutiny**: The level of regulatory scrutiny can vary by jurisdiction and product, influencing the techniques used. For instance, personal auto and workers' compensation are highly regulated in the U.S..  
* **Competitive Environment**: The final rates implemented in practice may differ from actuarial indications due to market conditions, competitive pressures, and price demand elasticity. Companies must balance cost-based indications with marketing considerations to optimize profitability and volume objectives.  
* **Operational Constraints**: System limitations can hinder the implementation of new or refined rating variables. A cost-benefit analysis may be performed to weigh the tangible and intangible benefits of system changes against their costs.  
* **Softer Factors**: Actuarial estimates are often adjusted by management to account for "softer factors" like subtle changes in terms, risk management changes, or perceived market positions. This introduces subjectivity but can be crucial for practical implementation.

In summary, while the Burning Cost Approach offers a straightforward method for determining risk premiums, its practical application necessitates careful consideration of data quality, comprehensive adjustments for trends and development, specific handling of large claims, and an awareness of broader market and operational contexts. It serves as a foundational tool, but its utility is maximized when coupled with astute actuarial judgment and a clear understanding of its inherent limitations and required enhancements.

